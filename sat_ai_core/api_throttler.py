"""
sat_ai_core/api_throttler.py
-----------------------------------
B·ªô ƒëi·ªÅu ti·∫øt (throttler) v√† c∆° ch·∫ø retry n√¢ng cao cho c√°c l·ªánh g·ªçi OpenAI API.
Tr√°nh l·ªói HTTP 429 ("Too Many Requests") ho·∫∑c l·ªói m·∫°ng t·∫°m th·ªùi.

‚úÖ ƒêi·ªÉm n·ªïi b·∫≠t:
- Gi·ªõi h·∫°n t·ªëc ƒë·ªô theo model ho·∫∑c to√†n c·ª•c (per-model throttling)
- T·ª± ƒë·ªông retry v·ªõi backoff theo c·∫•p s·ªë nh√¢n + jitter
- T√¥n tr·ªçng header Retry-After c·ªßa OpenAI (n·∫øu c√≥)
- Ph√¢n bi·ªát l·ªói t·∫°m th·ªùi (retry ƒë∆∞·ª£c) v√† l·ªói vƒ©nh vi·ªÖn (ng·ª´ng retry)
- Thread-safe, kh√¥ng l√†m ngh·∫Ωn lu·ªìng kh√°c
- Logging r√µ r√†ng, c√≥ th·ªÉ t√≠ch h·ª£p v√†o h·ªá th·ªëng gi√°m s√°t
"""

import time
import random
import logging
from threading import Lock
from typing import List, Dict, Any, Optional
from openai import OpenAI
from openai import RateLimitError, APIError, APITimeoutError

# ==============================
# ‚öôÔ∏è C·∫•u h√¨nh logging
# ==============================
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter("[%(asctime)s] %(levelname)s - %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)


# ==============================
# üß© L·ªõp Exception t√πy bi·∫øn
# ==============================
class ThrottlerError(Exception):
    """B√°o l·ªói khi h·∫øt l∆∞·ª£t retry ho·∫∑c API li√™n t·ª•c th·∫•t b·∫°i."""

    def __init__(self, message: str, last_exception: Optional[BaseException], attempts: int):
        super().__init__(message)
        self.last_exception = last_exception
        self.attempts = attempts


# ==============================
# üöÄ L·ªõp ApiThrottler (b·∫£n c·∫£i ti·∫øn)
# ==============================
class ApiThrottler:
    def __init__(
        self,
        min_interval: float = 2.0,
        max_retries: int = 5,
        max_wait: float = 30.0,
        per_model: bool = True,
    ):
        """
        Tham s·ªë:
            min_interval: Kho·∫£ng c√°ch t·ªëi thi·ªÉu gi·ªØa 2 l·∫ßn g·ªçi API (gi√¢y)
            max_retries: S·ªë l·∫ßn retry t·ªëi ƒëa
            max_wait: Th·ªùi gian ch·ªù t·ªëi ƒëa gi·ªØa c√°c l·∫ßn retry
            per_model: Gi·ªõi h·∫°n ri√™ng theo t·ª´ng model (True) ho·∫∑c to√†n c·ª•c (False)
        """
        self.min_interval = min_interval
        self.max_retries = max_retries
        self.max_wait = max_wait
        self.per_model = per_model

        self._lock = Lock()
        self._last_call: Dict[str, float] = {}

    # ------------------------------
    # üîß X·ª≠ l√Ω th·ªùi gian an to√†n
    # ------------------------------
    def _now(self) -> float:
        return time.monotonic()

    def _key(self, model: str) -> str:
        return model if self.per_model else "__global__"

    # ------------------------------
    # ‚è≥ Ch·ªù slot an to√†n (thread-safe)
    # ------------------------------
    def _wait_for_slot(self, key: str):
        with self._lock:
            now = self._now()
            last = self._last_call.get(key, 0.0)
            elapsed = now - last
            if elapsed < self.min_interval:
                wait = self.min_interval - elapsed
                logger.debug(f"‚è≥ Ch·ªù {wait:.2f}s ƒë·ªÉ tr√°nh v∆∞·ª£t gi·ªõi h·∫°n API ({key})")
                self._lock.release()
                try:
                    time.sleep(wait)
                finally:
                    self._lock.acquire()
            self._last_call[key] = self._now()

    # ------------------------------
    # üß† T√≠nh to√°n th·ªùi gian backoff
    # ------------------------------
    def _compute_backoff(self, attempt: int, retry_after: Optional[float]) -> float:
        if retry_after is not None:
            return min(self.max_wait, max(0.0, retry_after))
        return min(self.max_wait, 2 ** attempt + random.uniform(0.5, 2.0))

    # ------------------------------
    # üì• H√†m ch√≠nh: g·ªçi API an to√†n
    # ------------------------------
    def safe_openai_chat(
        self,
        client: OpenAI,
        messages: List[Dict[str, Any]],
        model: str = "gpt-4o-mini",
        **kwargs,
    ):
        """
        G·ªçi API OpenAI v·ªõi throttling + retry t·ª± ƒë·ªông.
        Tr·∫£ v·ªÅ response n·∫øu th√†nh c√¥ng, n√©m ThrottlerError n·∫øu th·∫•t b·∫°i sau N l·∫ßn.
        """
        key = self._key(model)
        last_exc: Optional[BaseException] = None

        for attempt in range(1, self.max_retries + 1):
            self._wait_for_slot(key)

            try:
                response = client.chat.completions.create(model=model, messages=messages, **kwargs)
                return response

            # ----- X·ª≠ l√Ω l·ªói gi·ªõi h·∫°n -----
            except RateLimitError as e:
                retry_after = self._get_retry_after(e)
                wait_time = self._compute_backoff(attempt, retry_after)
                logger.warning(f"‚ö†Ô∏è Rate limit (HTTP 429). Ch·ªù {wait_time:.1f}s tr∆∞·ªõc khi retry ({attempt}/{self.max_retries})")
                time.sleep(wait_time)
                last_exc = e

            # ----- L·ªói timeout -----
            except APITimeoutError as e:
                wait_time = self._compute_backoff(attempt, None)
                logger.warning(f"‚è±Ô∏è Timeout API. Ch·ªù {wait_time:.1f}s r·ªìi retry ({attempt}/{self.max_retries})")
                time.sleep(wait_time)
                last_exc = e

            # ----- L·ªói m√°y ch·ªß (5xx) -----
            except APIError as e:
                status = getattr(e, "status_code", None)
                if status and 500 <= status < 600:
                    wait_time = self._compute_backoff(attempt, None)
                    logger.warning(f"üí• L·ªói m√°y ch·ªß ({status}). Ch·ªù {wait_time:.1f}s r·ªìi retry ({attempt}/{self.max_retries})")
                    time.sleep(wait_time)
                    last_exc = e
                else:
                    logger.error(f"üö´ L·ªói API kh√¥ng th·ªÉ retry ({status}): {e}")
                    raise

            # ----- C√°c l·ªói kh√°c -----
            except Exception as e:
                logger.error(f"üö® L·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi OpenAI: {e}")
                last_exc = e
                break

        # N·∫øu h·∫øt l∆∞·ª£t retry
        raise ThrottlerError("‚ùå H·∫øt l∆∞·ª£t retry ‚Äî API th·∫•t b·∫°i.", last_exc, self.max_retries)

    # ------------------------------
    # üîç H√†m ph·ª• l·∫•y Retry-After
    # ------------------------------
    def _get_retry_after(self, exc: Exception) -> Optional[float]:
        try:
            headers = getattr(exc, "response", None)
            if headers and hasattr(headers, "headers"):
                val = headers.headers.get("Retry-After")
                if val:
                    return float(val)
        except Exception:
            pass
        return None
