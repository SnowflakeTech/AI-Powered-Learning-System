"""
sat_ai_core/ai_explainer.py
-----------------------------------
Module gi·∫£i th√≠ch c√¢u tr·∫£ l·ªùi SAT b·∫±ng m√¥ h√¨nh OpenAI (v√≠ d·ª• gpt-4o-mini).
Bao g·ªìm caching SQLite, streaming hi·ªÉn th·ªã tr√™n CLI, v√† ƒë·ªãnh d·∫°ng Markdown.
"""

import os
import time
import re
import hashlib
import sqlite3
import logging
from openai import OpenAI
from dotenv import load_dotenv

# ============ KH·ªûI T·∫†O ============
env_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), ".env")
load_dotenv(dotenv_path=env_path)
logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s - %(message)s")

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("‚ùå OPENAI_API_KEY ch∆∞a ƒë∆∞·ª£c set trong .env!")

model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
client = OpenAI(api_key=api_key)

# ============ C·∫§U H√åNH DATABASE CACHE ============
DB_PATH = "ai_cache.db"
os.makedirs(os.path.dirname(DB_PATH) or ".", exist_ok=True)

def _init_db():
    conn = sqlite3.connect(DB_PATH)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS cache (
            key TEXT PRIMARY KEY,
            response TEXT NOT NULL
        );
    """)
    conn.commit()
    conn.close()

def _get_cache(key: str):
    conn = sqlite3.connect(DB_PATH)
    row = conn.execute("SELECT response FROM cache WHERE key=?", (key,)).fetchone()
    conn.close()
    return row[0] if row else None

def _set_cache(key: str, text: str):
    conn = sqlite3.connect(DB_PATH)
    conn.execute("INSERT OR REPLACE INTO cache VALUES (?, ?)", (key, text))
    conn.commit()
    conn.close()

_init_db()

# ============ H√ÄM X·ª¨ L√ù VƒÇN B·∫¢N ============

def _format_response(raw: str, correct_choice: str) -> str:
    """ƒê·ªãnh d·∫°ng vƒÉn b·∫£n ƒë·∫ßu ra th√†nh Markdown 3 ph·∫ßn: t√≥m t·∫Øt, b∆∞·ªõc gi·∫£i, k·∫øt lu·∫≠n"""
    raw = raw.strip()
    summary, steps, conclusion = "", "", ""

    parts = re.split(r"1|2|3|T√≥m|B∆∞·ªõc|K·∫øt", raw, flags=re.IGNORECASE)
    if len(parts) >= 3:
        summary, steps, conclusion = parts[:3]
    else:
        segs = raw.split(".")
        if len(segs) > 2:
            summary, steps, conclusion = segs[0], " ".join(segs[1:-1]), segs[-1]
        else:
            summary = raw

    step_lines = re.split(r";|\n|‚Ä¢|-|\*", steps)
    step_lines = [s.strip() for s in step_lines if s.strip()]
    steps_md = "\n".join([f"- {s}" for s in step_lines])

    # Chuy·ªÉn c√°c ph√©p t√≠nh sang LaTeX `$...$`
    for target in [summary, steps_md, conclusion]:
        target = re.sub(r"(\d+\s*[+\-*/=]\s*\d+)", r"$\1$", target)

    state = "‚úÖ ƒê√öNG" if correct_choice in raw else "‚ùå SAI"

    return f"""
1 **T√≥m t·∫Øt ƒë·ªÅ:**
{summary.strip()}

2 **C√°c b∆∞·ªõc ch√≠nh:**
{steps_md.strip()}

3 **K·∫øt lu·∫≠n ({state}):**
{conclusion.strip()}
""".strip()


# ============ H√ÄM CH√çNH ============

def explain_answer(question: str, correct_choice: str) -> str:
    """
    Gi·∫£i th√≠ch c√¢u h·ªèi SAT b·∫±ng OpenAI.
    C√≥ cache ƒë·ªÉ tr√°nh g·ªçi l·∫°i API nhi·ªÅu l·∫ßn.
    """
    prompt = f"""
B·∫°n l√† gia s∆∞ SAT. H√£y gi·∫£i th√≠ch ng·∫Øn g·ªçn b·∫±ng Markdown, g·ªìm 3 ph·∫ßn:
1 T√≥m t·∫Øt ƒë·ªÅ
2 C√°c b∆∞·ªõc gi·∫£i
3 K·∫øt lu·∫≠n
---
C√¢u h·ªèi: {question}
ƒê√°p √°n ƒë√∫ng: {correct_choice}
"""

    key = hashlib.sha256(prompt.encode()).hexdigest()
    cached = _get_cache(key)
    if cached:
        print("‚ö° ƒê√£ c√≥ cache, kh√¥ng g·ªçi l·∫°i API.\n")
        print(cached)
        return cached

    print(f"\nüìò ƒêang gi·∫£i th√≠ch c√¢u h·ªèi b·∫±ng {model}...\n")

    stream = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "B·∫°n l√† gia s∆∞ SAT chuy√™n gi·∫£i th√≠ch r√µ r√†ng v√† ch√≠nh x√°c."},
            {"role": "user", "content": prompt},
        ],
        stream=True,
        temperature=0.6,
    )

    full_text = ""
    token_count = 0

    for chunk in stream:
        delta = chunk.choices[0].delta
        text = getattr(delta, "content", None)
        if text:
            print(text, end="", flush=True)
            full_text += text
            token_count += len(text.split())
            time.sleep(0.003)

    print("\n\n‚úÖ Ho√†n t·∫•t gi·∫£i th√≠ch!")
    logging.info(f"üìä Tokens ∆∞·ªõc l∆∞·ª£ng: {token_count}")

    formatted = _format_response(full_text, correct_choice)
    print("\nüéØ K·∫øt qu·∫£ format:\n")
    print(formatted)

    _set_cache(key, formatted)
    return formatted


# ============ DEMO ============
if __name__ == "__main__":
    q = "M·ªôt h√¨nh ch·ªØ nh·∫≠t c√≥ chi·ªÅu d√†i g·∫•p ƒë√¥i chi·ªÅu r·ªông. Chu vi l√† 36 th√¨ di·ªán t√≠ch l√† bao nhi√™u?"
    a = "81"
    explain_answer(q, a)
